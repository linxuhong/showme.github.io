# 系统想着

### Dns

1 在浏览器中输入www.xxx.com 域名，
   
    操作系统会先检查自己本地的hosts文件是否有这个网址映射关系，
    如果有，就先调用这个IP地址映射，完成域名解析。

2 如果hosts里没有这个域名的映射，则查找本地DNS解析器缓存，
   
    是否有这个网址映射关系，如果有，直接返回，完成域名解析。

3 如果hosts与本地DNS解析器缓存都没有相应的网址映射关系，
   
    首先会找TCP/ip参数中设置的首选DNS服务器，在此我们叫它本地DNS服务器，
    此服务器收到查询时，如果要查询的域名，包含在本地配置区域资源中，
    则返回解析结果给客户机，完成域名解析，此解析具有权威性。

4 如果要查询的域名，不由本地DNS服务器区域解析，但
   
    该服务器已缓存了此网址映射关系，则调用这个IP地址映射，完成域名解析，此解析不具有权威性。

5 如果本地DNS服务器本地区域文件与缓存解析都失效，
   
    则根据本地DNS服务器的设置（是否设置转发器）进行查询，如果未用转发模式，本地DNS就把请求发至13台根DNS，根DNS服务器收到请求后会判断这个域名(.com)是谁来授权管理，并会返回一个负责该顶级域名服务器的一个IP。本地DNS服务器收到IP信息后，将会联系负责.com域的这台服务器。这台负责.com域的服务器收到请求后，如果自己无法解析，它就会找一个管理.com域的下一级DNS服务器地址(http://www.xx.com)给本地DNS服务器。当本地DNS服务器收到这个地址后，
    xx就会找http://www.xx.com域服务器，重复上面的动作，进行查询，
    晨直至找到www.xx.com主机。

6 如果用的是转发模式，此DNS服务器就会把请求转发至上一级DNS服务器，
    
    由上一级服务器进行解析，上一级服务器如果不能解析，
    或找根DNS或把转请求转至上上级，以此循环。
    不管是本地DNS服务器用是是转发，
    还是根提示，最后都是把结果返回给本地DNS服务器，
    由此DNS服务器再返回给客户机。


DNS记录类型
1 A记录（Address）
   
    A 记录也称为主机记录，是使用最广泛的DNS记录，A记录的基本作用就是说明一个域名对应的IP是多少， 
    它是域名和IP地址的对应关系，表现形式为 www.baidu.com 119.75.217.109 , 
    这就是一个A记录！A记录除了进行域名IP对应以外，
    还有一个高级用法，可以作为低成本的负载均衡的解决方案，
    比如说，www.baidu.com可以创建多个A记录，对应多台物理服务器的IP地址，可以实现基本的流量均衡！)。

2 NS 记录（Name Server）

    NS 记录和SOA记录是任何一个DNS区域都不可或缺的两条记录，NS记录也叫名称服务器记录，
    用于说明这个区域有哪些DNS服务器负责解析，SOA记录说明负责解析的DNS服务器中哪一个是主服务器。
    因此，任何一个DNS区域都不可能缺少这两条记录。NS记录，
    说明了在这个区域里，有多少个服务器来承担解析的任务。
3 SOA 记录（Start ofAuthority）

    NS 记录说明了有多台服务器在进行解析，但哪一个才是主服务器呢，
    NS并没有说明，这个就要看SOA记录了，
    SOA名叫起始授权机构记录，
    SOA记录说明了在众多NS记录里那一台才是主要的服务器。

4 MX 记录（ Mail Exchanger ）

    全称是邮件交换记录，在使用邮件服务器的时候，
    MX记录是无可或缺的，比如A用户向B用户发送一封邮件，
    那么他需要向ＤＮＳ查询Ｂ的MX记录，
    DNS在定位到了B的MX记录后反馈给A用户，
    然后Ａ用户把邮件投递到B用户的ＭＸ记录服务器里。
5 CNAME 记录

    别名记录。这种记录允许您将多个名字映射到另外一个域名。
    通常用于同时提供WWW和MAIL服务的计算机。
    例如，有一台计算机名为“host.mydomain.com”（A记录）。
    它同时提供WWW和MAIL服务，
    为了便于用户访问服务。可以为该计算机设置两个别名（CNAME）：WWW和MAIL。
    这两个别名的全称就http://www.mydomain.com/ 
    和“mail.mydomain.com”。实际上他们都指向“host.mydomain.com”。

6 SRV 记录

    SRV 记录的作用是指明某域名下提供的服务。
    例如：
    _http._tcp.example.com. SRV 10 5 80. www.example.com
    SRV 后面项目的含义：
    
    10 – 优先级，类似 MX 记录
    5 – 权重
    80 – 端口

    www.example.com – 实际提供服务的主机名。
7 PTR 记录

    PTR 记录也被称为指针记录， PTR 记录是 A 记录的逆向记录，作用是把 IP 地址解析为域名。由于我们在前面提到过， DNS 的反向区域负责从 IP 到域名的解析，因此如果要创建 PTR 记录，必须在反向区域中创建。
    ip 反查域名： https://dns.aizhan.com/103.43.134.210/
         
### 负载均衡 

1.  OSPF DLVS DR
   
    它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、
    提高网络的灵活性和可用性

2. LVS  
三种LVS负载均衡模式


调度器的实现技术中,IP负载均衡技术是效率最高的,IP虚拟服务器软件(IPVS)是在linux内核中实现的｡

 1.NAT模式

    NAT用法本来是因为网络IP地址不足而把内部保留IP地址通过映射转换成公网地址的一种上网方式(原地址NAT)｡
    如果把NAT的过程稍微变化,就可以成为负载均衡的一种方式｡
    原理其实就是把从客户端发来的IP包的IP头目的地址在DR上换成其中
    一台REALSERVER的IP地址并发至此REALSERVER,而REALSERVER则在处理完成后把数据经过DR主机发回给客户端,
    DR在这个时候再把数据包的原IP地址改为DR接口上的IP地址即可｡期间,
    无论是进来的流量,还是出去的流量,都必须经过DR｡

 2.IP隧道模式


    隧道模式则类似于VPN的方式,使用网络分层的原理,在从客户端发来的数据包的基础上,
    封装一个新的IP头标记(不完整的IP头,只有目的IP部)发给REALSERVER,REALSERVER收到后,
    先把DR发过来的数据包的头给解开,还原其数据包原样,处理后,直接返回给客户端,
    而不需要再经过DR｡需要注意的是,由于REALSERVER需要对DR发过来
    的数据包进行还原,也就是说必须支持IPTUNNEL协议｡所以,在REALSERVER的内核中,
    必须编译支持IPTUNNEL这个选项｡IPTUNNEL也在Net working options里面｡

 3.直接路由模式

    直接路由模式比较特别,很难说和什么方面相似,前2种模式基本上都是工作在网络层上(三层),
    而直接路由模式则应该是工作在数据链路层上(二层)｡其原理为,DR和REALSERVER都使用同一个IP对外服务｡
    但只有DR对ARP请求进行响应,所有REALSERVER对本身这个IP的ARP请求保持静默｡也就是说,网关会把对这个
    服务IP的请求全部定向给DR,而DR收到数据包后根据调度算法,找出对应的REALSERVER,
    把目的MAC地址改为REALSERVER的MAC并发给这台REALSERVER｡这时REALSERVER收到这个数据包,
    则等于直接从客户端收到这个数据包无异,处理后直接返回给客户端｡由于DR要对二
    层包头进行改换,所以DR和REALSERVER之间必须在一个广播域,也可以简单的理解为在同一台交换机上｡

LVS负载均衡的八种调度算法

 1.轮叫调度(Round-RobinScheduling)

    调度器通过"轮叫"调度算法将外部请求按顺序轮流分配到集群中的真实服务器上,它均等地对待每一台服务器,
    而不管服务器上实际的连接数和系统负载｡

 2.加权轮叫调度(WeightedRound-RobinScheduling)


    调度器通过"加权轮叫"调度算法根据真实服务器的不同处理能力来调度访问请求｡
    这样可以保证处理能力强的服务器处理更多的访问流量｡调度器可以自动问询真实服务器的负载情况,
    并动态地调整其权值｡

 3.最小连接调度(Least-ConnectionScheduling)

    调度器通过"最少连接"调度算法动态地将网络请求调度到已建立的链接数最少的服务器上｡
    如果集群系统的真实服务器具有相近的系统性能,采用"最小连接"调度算法可以较好地均衡负载｡

 4.加权最小连接调度(WeightedLeast-ConnectionScheduling)

    在集群系统中的服务器性能差异较大的情况下,调度器采用"加权最少链接"
    调度算法优化负载均衡性能,具有较高权值的服务器将承受较大比例的活动连接负载｡
    调度器可以自动问询真实服务器的负载情况,并动态地调整其权值

 5.基于局部性的最少链接(Locality-BasedLeastConnectionsScheduling)

    基于局部性的最少链接"调度算法是针对目标IP地址的负载均衡,
    目前主要用于Cache集群系统｡该算法根据请求的目标IP地址找出该目标IP地址最近使用的服务器,
    若该服务器是可用的且没有超载,将请求发送到该服务器;若服务器不存在,或者该服务器超载且有服务器处于一
    半的工作负载,则用"最少链接"的原则选出一个可用的服务器,将请求发送到该服务器｡

 6.带复制的基于局部性最少链接(Locality-BasedLeastConnectionswithReplicationScheduling)

    带复制的基于局部性最少链接"调度算法也是针对目标IP地址的负载均衡,目前主要用于Cache集群系统｡它与LBLC算法的不同之处是它要维护从一个目标IP地址到一组服务器的映射,而LBLC算法维护从一个目标IP地址到一台服务器的映射｡
    该算法根据请求的目标IP地址找出该目标IP地址对应的服务器组,按"最小连接"原则从服务器组中选出一台服务器,若服务器没有超载,将请求发送到该服务器,若服务器超载;则按"最小连接"原则从这个
    集群中选出一台服务器,将该服务器加入到服务器组中,将请求发送到该服务器｡同时,
    当该服务器组有一段时间没有被修改,将最忙的服务器从服务器组中删除,以降低复制的程度

 7.目标地址散列调度(DestinationHashingScheduling)

    目标地址散列"调度算法根据请求的目标IP地址,作为散列键(HashKey)从静态分配的散列表找出对应的服务器,
    若该服务器是可用的且未超载,将请求发送到该服务器,否则返回空

 8.源地址散列调度(SourceHashingScheduling)

    源地址散列"调度算法根据请求的源IP地址,作为散列键(HashKey)从静态分配的
 
 
### 分布式事务

1.  分布式事务简介

  说到事务，大家第一时间想到的应该是数据库的事务，数据库的事务保证了同一数据库事务中的多个操作要么都成功，要么都失败。但是如何保证不同数据库的多个操作要么都成功要么都失败呢？这时引出了“分布式事务”的概念。
        
        提到分布式事务，我们就一定要说到XA协议、2PC和3PC，下面我们来详细介绍下这部分的内容。

1.1.   XA协议

    X/Open组织定义了分布式事务处理模型，即X/Open DTP模型。此模型中定义了应用程序（Application，简称AP）、全局事务管理器（Transaction Manager，简称TM）和本地资源管理器（Resource Manager,简称RM）。
    AP：指我们的业务程序，是事务的发起方；
    TM：在多台机器上理论上无法达到一致的状态，如果需要达到一致的状态就需要引入一个协调者，TM即是这个协调者，它控制着全局的事务，管理事务的生命周期，协调资源；
    RM：控制并管理实际的资源，一般指的是一个全局分布式事务中的分支事务的执行者，例如数据库、消息中间件等。

    XA协议主要定义了TM和RM之间的接口规范。XA接口是双向接口，通过XA协议可以在TM与一个或多个RM之间建立通信桥梁。
 
1.2.   2PC(二阶段提交)
    
    二阶段提交，顾名思义，即将分布式事务的提交分成两个阶段，示意图如下：

1.2.1.   第一阶段（预备阶段）

    在预备阶段，TM向所有的RM发送prepare消息，等等RM的执行，RM根据自身的情况执行分支事务，并做好redo和undo记录，然后回复结果给TM。

    RM可能的回复，以及针对此回复事务的处理：
    
    READY：表示准备就绪，等待正式提交。当所有的RM都回复READY时，整个事务将在第二阶段进行提交。
    READ_ONLY：表示提交跟我没关系，我是只读的，回复READ_ONLY的RM在第二阶段会被忽略掉。
    NOT_READY：表示未准备好或者出错，当有任何一个RM回复的是NOT_READY时，整个事务将在第二阶段进行回滚。
    没有返回值：当RM宕机或者网络问题导致TM收不到RM的回复时，整个事务将在第二阶段进行回滚。


1.2.2.   第二阶段（提交阶段）

    在提交阶段TM根据第一阶段所有RM的返回结果，向所有的RM发送commit或rollback指令，各RM对自己的分支事务进行commit或rollback操作。



1.2.3.   2PC的问题

    同步阻塞问题，2pc的整个过程中，所有RM都是阻塞的，在整个事务处理完成之前会一致占用着相关资源，整体性能形成木桶短板效应。并且，如果因为网络问题，某一RM没有收到commit或rollback指令时，此RM将永远处于阻塞状态。
    单点故障问题，如果在事务执行的过程中TM因为宕机或者网络问题无法来协调所有的RM，那么RM将一直处于阻塞状态，无法释放资源。
    数据一致性问题，如果在事务的第二阶段，有的RM因为宕机或者网络问题无法接收TM发来的事务提交指令造成分支事务没有提交，而其他RM正常提交分支事务，就会造成数据不一致。


1.3.   3PC(三阶段提交)
    
    针对2PC的单点故障问题引发或网络问题引发的RM一直处于阻塞状态，3PC引入了超时机制，并在原2PC的第一阶段拆分为两个阶段，来保证在真正的提交阶段各RM的状态是一致的。示意图如下：



1.3.1.   第一阶段(询问阶段)
    
    询问阶段主要是做的个RM的状态检查，验证各RM的状态以及和TM的通信状态，但所有RM都返回YES后进入第二阶段。否则事务结束。



1.3.2.   第二阶段（预提交阶段）

    第二阶段类似2PC的准备阶段。TM想所有RM发送prepare消息，RM执行分支事务，并做好redo和undo记录，并给TM ack响应。
    
    成功，所有RM响应成功，整个事务将进入第三阶段体检。
    失败，有任何一个RM响应失败，整个事务进入第三阶段回滚。
    无响应，TM在超过超时时间后如果有任何一个RM无响应，整个事务进行回滚。
    

1.3.3.   第三阶段（提交阶段）

    在提交阶段，TM根据第二阶段的结果想所有RM发送commit或rollback指令，各RM对自己的分支事务进行commit或rollback操作。
    
    在此阶段，如果RM超过一定时间没有接收到TM的commit或rollback指令，将自动执行commit操作。

1.3.4.   3PC的问题

    3PC虽然引入了RM和TM的双向超时机制，只是增加了事务正确处理的概率。以及减少资源被长期占有不能释放，但仍无法避免因为宕机或者网络产生的数据不一致问题。
    


1.4.  总结

    2PC和3PC实现了XA协议，但任然无法解决实际过程中的问题，并且目前支持XA协议的服务很少，主要都是数据库（rocketMq也支持XA）。
    
    但是在互联网、电商行业微服务化的结构体系下，只是数据库的分布式事务的业务场景非常之少（先不论数据库的分布式事务的各种问题），并且在SOA体系下，数据是各微服务私有的，访问微服务只能通过API，在整个调用链中甚至是跨平台跨语言的。数据库+微服务，微服务+微服务等这类的分布式事务场景要如何做呢。


注：微服务是泛称，包含各种微服务框架的微服务、RESTFUL接口、各种中间件基础组件（如mq、redis等等）等。



2.  SOA框架下分布式业务场景的思考
    
   例如有这样一个场景，你是电商下单系统的研发，现在的需求是用户下单，在下单系统中需要保存用户的订单数据并调用库存系统的服务来预占库存。
    
        你要如何设计才能保证这两步操作的原子性呢。
    
    
    
    可能大家第一想到的思路大概是这样的，在数据库保存订单数据的事务中调用库存服务的接口，如果调用库存服务成功，提交保存订单的事务，如果调用库存服务失败，则回滚保存订单的事务，大致的伪代码如下：

        {
        
               开启事务；
        
               try{
        
                      数据库保存订单数据；
        
        
        
                      调用库存系统服务预占库存；
        
        
        
                      提交事务；
        
               }catch(异常){
        
                      回滚事务；
        
               }
        
        }


    
    这个方案乍一看貌似没有什么问题，但仔细思考下还是有很多问题的，主要有如下两个问题：
    
    将外部的调用方在数据库的事务中，会增加数据库锁的占用时间，严重影响了数据库的吞吐量。
    最坏的情况是外部调用没有设置超时时间，或者超时时间很长。
    调用外部服务异常就一定表示外部服务处理失败了吗？
    如果外部服务处理成功只是因为网络问题或者超时没有收到外部服务的反馈呢。


     之所以我们的第一反应是这种方案，在于在我们的思想中事务是要遵循ACID原则的，
     但是在微服务这样复杂的环境下，盲目的追寻ACID会让我们的系统非常复杂，
     并且不那么健壮，所以在微服务体系下，针对分布式事务的业务场景，应该追寻的是数据的最终一致性。

3.  最终数据一致性方案
3.1.  使用消息中间件方案

  消息中间件模式，是指我们将事务发送到消息队列，各分支事务执行者分别监听MQ消息进行各自的处理。在此模式下，下单占库存的处理流程就会变为这样：
        
        下单服务接收用户下单请求，将请求信息发送到MQ队列中，返回发送是否成功
        保存订单服务接收下单请求消息保存订单，如果消息消费失败，消息会再次被接收进行重试。
        库存预占服务接收下单清晰消息，预占库存，如果消息消费失败，消息会再次被接收进行重试。

这个方案完美吗，当然不，至少有一下几个风险点：
    
    a)     数据已发送到MQ，但因为系统宕机或者网络故障没有收到MQ的成功状态，导致给调用方返回的是失败或者超时，调用发重新发送请求，造成消息重复。
    
    b)    插入订单成功但是因系统宕机或者网络异常没有给MQ返回消息消费成功，造成消息重复消费，但重复消费时因插入冲突造成一直失败一直重试。
    
    c)     预占库存成功，但因为系统宕机或者网络异常没有给MQ发挥消息消费成功，造成消息重复消费，多预占了库存。
    
    d)    预占库存时发现库存不足，MQ消费成功但业务是失败的，但订单已插入。
    
                                
    针对这些问题，我们还要在基于消息中间件的整个事务之外增加一些东西，比如针对问题a)/b)/c)我们要求订单插入服务和库存预占服务要支持幂等就可解决；针对问题d)，当发现库存不足时，再向另一个MQ队列中发送一个通知订单取消的消息，用另一个服务来接收此消息进行订单的取消操作。
    使用消息中间件还有另外一个好处，就是当请求量大的时候用消息中间件来削峰，保证系统的稳定。但另一个不确定性就是要依赖消息中间件的稳定性，要保证不丢消息。

3.2.  使用事务状态表+后台任务方案

    事务状态表模式是指，在事务开始之前先初始化一个各分支事务的初始化状态，然后使用后台任务来扫描整个事务状态表，并去执行各分支任务。在此模式下，下单占库存的处理流程就会变成这样：
    
    下单服务接收到下单请求后，在本地数据库事务下向事务状态表（可以是一个表也可以是两个表）中插入保存订单分支事务并标记初始状态（状态集可以定义为：初始状态、成功状态、失败许重试、失败等）；插入调用库存服务预占库存分支事务并标记初始状态。
    使用后台任务定时扫描事务状态表，获取其中状态不是成功和失败的中间状态的分支事务并执行，根据执行的反馈结果更新事务状态表，并做相应的处理。例如执行分支任务反馈的是超时，可以将状态更新为失败需重试，等待下次重新执行，反馈的是失败，那么将同一全局事务下的其他已成功的分支事务回滚。
    
    这个方案完美吗？看似没什么大问题，但仍有几个风险点：
    
    a)     如果分支事务因超时反馈失败，下次重试造成重复执行。
    
    b)    如果进行回滚。
    
    c)     后台任务挂了怎么办，事务将一直无法执行，或处于数据不一致的状态。
    
    针对问题a)，我们需要分支事务执行方要实现幂等；针对问题b)我们需要分支事务执行方提供回滚服务；针对问题c)我们要求后台服务不是单点。
    使用此方案的一个好处是我们可以自己控制请求不会丢，但请求量大的时候数据库操作容易成为瓶颈，并且事务的执行时间取决于后台任务的执行间隔。



3.3.  妥协+对账方案

    为什么说是妥协呢，就是对最终一致性我们也妥协，使用弱一致性，对于下单占库存的场景来说，超卖是我们不允许的，多占了库存导致少买我们是可以容忍的，只要最后能将多占的这部分库存释放就可以，释放的时间要求没那么高，在这种模式下，下单占库存的处理流程就会变成下面这样：
    
    下单服务收到下单请求后调用库存服务进行预占库存
    如果占库存失败，返回给调用方失败。
    如果占用库存成功，进行订单保存操作，将保存成功与否的结果返回给调用方，就是保存失败也不进行预占库存的回滚。
    系统在一定时间间隔后核算占用明细，将不存在的订单号的占用释放掉


这个方案完美吗？当然不，他存在的问题是，在一定的时间内数据是不一致的，但是在这个业务场景下这个方案是可行的，而且因为是同步的，调用方的体验会很好。

3.4.  TCC方案

    上面提到的2PC/3PC指的是数据库层面，TCC（try/commit/cancel）是支付宝开源的SOA层面的2PC方案，使用此方案需要引入一个TCC框架，其执行流程如下：
    
    Try阶段，通知各子事务RM进行资源的预留，比如冻结库存、冻结余额等，例如增加预占库存量的字段和预占余额的字段。
    Commit阶段，进行事务的执行，在经过了try阶段，只是将冻结的资源更新到真实扣减或增加的字段上，大概率的能成功。就算失败，框架因为有记录个分支事务的状态会一直重试，最终成功。
    Cancel阶段，主要是针对try阶段资源预留失败，通知其他RM释放预占的资源。
    当然这对各分支事务执行者也有要求，首先各分子业务参与者都有开发量，分别提供try/commit/cancel的接口及实现，并且接口要实现幂等，而失败重试，状态控制都由TCC框架来控制。
 

4.  总结

  虽然在上面罗列了一些分布式事务场景下的一些方案，但是在微服务框架下，没有哪一个方案是完美的，没有哪一个方案能适用于所有场景的，也没有哪一个场景能使用所有方案。
    
        比如我们消息中间件的方案，在用在下单占库存的场景下，既要各分支事务执行方实现幂等，又要有补偿机制来做异常处理。但是如果这个方案用在“用户支付完成后，修改订单状态变更为已支付，真实的扣减库存”这样的场景，因为业务本身决定后面的两个分支流程不存在业务上的失败，所以我们只需要实现幂等，而不需要有补偿机制。

        所以，总结上面的几个方案，在SOA模式下，保证分布式事务的最终一致性，其关键在于：有效的事件触达、幂等、补偿机制、适当的妥协。
        
        l  有效的事件触达：即一定能通知到各分支事务执行方，让其做响应的操作，如上面例子中提到的，消息队列、状态表+后台任务等等。
        
        l  幂等：即在技术或业务上要保持幂等，如重复请求的处理等等。
        
        l  补偿机制：即在异常情况下对业务的补偿，如上面例子中的回滚机制、对账等等。
        
        l  适当的妥协：妥协说的是业务上的妥协，在满足场景需求的前提下，不要太过于追求完美。

        在有限的篇幅内无法列举所有的业务场景，也无法给出更多的解决方案，只希望通过这些内容，让大家了解些SOA模式下保证数据一致的一些思路，能给大家一些启发

### Nginx  

1.  常用配置参数   
    - include       mime.types;   #文件扩展名与文件类型映射表
    - default_type  application/octet-stream; #默认文件类型，默认为text/plain
    - access_log off; #取消服务日志   
    - log_format myFormat ' $remote_addr–$remote_user [$time_local] $request $status $body_bytes_sent $http_referer $http_user_agent $http_x_forwarded_for'; #自定义格式
    - access_log log/access.log myFormat;  #combined为日志格式的默认值
    - sendfile on;   #允许sendfile方式传输文件，默认为off，可以在http块，server块，location块。
    - sendfile_max_chunk 100k;  #每个进程每次调用传输数量不能大于设定的值，默认为0，即不设上限。
    - keepalive_timeout 65;  #连接超时时间，默认为75s，可以在http，server，location块。
    - proxy_connect_timeout 1;   #nginx服务器与被代理的服务器建立连接的超时时间，默认60秒
    - proxy_read_timeout 1; #nginx服务器想被代理服务器组发出read请求后，等待响应的超时间，默认为60秒。
    - proxy_send_timeout 1; #nginx服务器想被代理服务器组发出write请求后，等待响应的超时间，默认为60秒。
    - proxy_http_version 1.0 ; #Nginx服务器提供代理服务的http协议版本1.0，1.1，默认设置为1.0版本。
      - proxy_method get;    #支持客户端的请求方法。post/get；
    - proxy_ignore_client_abort on;  #客户端断网时，nginx服务器是否终端对被代理服务器的请求。默认为off。
    - proxy_ignore_headers "Expires" "Set-Cookie";  #Nginx服务器不处理设置的http相应投中的头域，这里空格隔开可以设置多个。
    - proxy_intercept_errors on;    #如果被代理服务器返回的状态码为400或者大于400，设置的error_page配置起作用。默认为off。
    - proxy_headers_hash_max_size 1024; #存放http报文头的哈希表容量上限，默认为512个字符。
    - proxy_headers_hash_bucket_size 128; #nginx服务器申请存放http报文头的哈希表容量大小。默认为64个字符。
    - proxy_next_upstream timeout;  #反向代理upstream中设置的服务器组，出现故障时，被代理服务器返回的状态值。error|timeout|invalid_header|http_500|http_502|http_503|http_504|http_404|off
    - proxy_ssl_session_reuse on; 默认为on，如果我们在错误日志中发现“SSL3_GET_FINSHED:digest check failed”的情况时，可以将该指令设置为off。

2. 代理 
    
    ~~~ xml
    upstream mysvr { 
    server 192.168.10.121:3333; 
    server 192.168.10.122:3333; } 
    server { .... location ~*^.+$ { 
    proxy_pass http://mysvr; #请求转向mysvr 定义的服务器列表  } }
    ~~~
    
-  热备：如果你有2台服务器，当一台服务器发生事故时，才启用第二台服务器给提供服务。服务器处理请求的顺序：AAAAAA突然A挂啦，BBBBBBBBBBBBBB.....
    
            ~~~ xml
            upstream mysvr { 
                server 127.0.0.1:7878; 
                server 192.168.10.121:3333 backup; #热备  
            }
            ~~~
    
-  轮询：nginx默认就是轮询其权重都默认为1，服务器处理请求的顺序：ABABABABAB....
  ~~~ xml
            upstream mysvr {
                 server 127.0.0.1:7878; 
                server 192.168.10.121:3333; 
            }
   ~~~
   
- 加权轮询：跟据配置的权重的大小而分发给不同服务器不同数量的请求。如果不设置，则默认为1。下面服务器的请求顺序为：ABBABBABBABBABB....
 ~~~ xml
        upstream mysvr { 
           server 127.0.0.1:7878 weight=1; 
            server 192.168.10.121:3333 weight=2; 
        }
   ~~~
   
- ip_hash:nginx会让相同的客户端ip请求相同的服务器。
 ~~~ xml
        upstream mysvr {
          server 127.0.0.1:7878; 
         server 192.168.10.121:3333; ip_hash; 
        }
   ~~~
        
- 如果你对上面4种均衡算法不是很理解，可以查看Nginx 配置详解，可能会更加容易理解点。

    到这里你是不是感觉nginx的负载均衡配置特别简单与强大，那么还没完，咱们继续哈，这里扯下蛋。

    关于nginx负载均衡配置的几个状态参数讲解。
    down，表示当前的server暂时不参与负载均衡。
    backup，预留的备份机器。当其他所有的非backup机器出现故障或者忙的时候，才会请求backup机器，因此这台机器的压力最轻。
    max_fails，允许请求失败的次数，默认为1。当超过最大次数时，返回proxy_next_upstream 模块定义的错误。
    fail_timeout，在经历了max_fails次失败后，暂停服务的时间。max_fails可以和fail_timeout一起使用。

 ~~~ xml
    upstream mysvr { 
        server 127.0.0.1:7878 weight=2 max_fails=2 fail_timeout=2; 
        server 192.168.10.121:3333 weight=1 max_fails=2 fail_timeout=1; 
    }

   ~~~
 
 
  
  
# License

* [Wiki]()

[]:https://wwww.pigasuo.com





























