# 系统想着


    
     
### 负载均衡 

1.  OSPF DLVS DR

 

2. LVS  
三种LVS负载均衡模式


调度器的实现技术中,IP负载均衡技术是效率最高的,IP虚拟服务器软件(IPVS)是在linux内核中实现的｡

 1.NAT模式

    NAT用法本来是因为网络IP地址不足而把内部保留IP地址通过映射转换成公网地址的一种上网方式(原地址NAT)｡
    如果把NAT的过程稍微变化,就可以成为负载均衡的一种方式｡原理其实就是把从客户端发来的IP包的IP头目的地址在DR上换成其中一台REALSERVER的IP地址并发至此REALSERVER,而REALSERVER则在处理完成后把数据经过DR主机发回给客户端,DR在这个时候再把数据包的原IP地址改为DR接口上的IP地址即可｡期间,
    无论是进来的流量,还是出去的流量,都必须经过DR｡

 2.IP隧道模式


    隧道模式则类似于VPN的方式,使用网络分层的原理,在从客户端发来的数据包的基础上,
    封装一个新的IP头标记(不完整的IP头,只有目的IP部)发给REALSERVER,REALSERVER收到后,
    先把DR发过来的数据包的头给解开,还原其数据包原样,处理后,直接返回给客户端,
    而不需要再经过DR｡需要注意的是,由于REALSERVER需要对DR发过来的数据包进行还原,也就是说必须支持IPTUNNEL协议｡所以,在REALSERVER的内核中,必须编译支持IPTUNNEL这个选项｡IPTUNNEL也在Net working options里面｡

 3.直接路由模式

    直接路由模式比较特别,很难说和什么方面相似,前2种模式基本上都是工作在网络层上(三层),
    而直接路由模式则应该是工作在数据链路层上(二层)｡其原理为,DR和REALSERVER都使用同一个IP对外服务｡
    但只有DR对ARP请求进行响应,所有REALSERVER对本身这个IP的ARP请求保持静默｡也就是说,网关会把对这个
    服务IP的请求全部定向给DR,而DR收到数据包后根据调度算法,找出对应的REALSERVER,把目的MAC地址改为REALSERVER的MAC并发给这台REALSERVER｡这时REALSERVER收到这个数据包,则等于直接从客户端收到这个数据包无异,处理后直接返回给客户端｡由于DR要对二层包头进行改换,所以DR和REALSERVER之间必须在一个广播域,也可以简单的理解为在同一台交换机上｡

LVS负载均衡的八种调度算法

 1.轮叫调度(Round-RobinScheduling)

    调度器通过"轮叫"调度算法将外部请求按顺序轮流分配到集群中的真实服务器上,它均等地对待每一台服务器,
    而不管服务器上实际的连接数和系统负载｡

 2.加权轮叫调度(WeightedRound-RobinScheduling)


    调度器通过"加权轮叫"调度算法根据真实服务器的不同处理能力来调度访问请求｡
    这样可以保证处理能力强的服务器处理更多的访问流量｡调度器可以自动问询真实服务器的负载情况,并动态地调整其权值｡

 3.最小连接调度(Least-ConnectionScheduling)

    调度器通过"最少连接"调度算法动态地将网络请求调度到已建立的链接数最少的服务器上｡
    如果集群系统的真实服务器具有相近的系统性能,采用"最小连接"调度算法可以较好地均衡负载｡

 4.加权最小连接调度(WeightedLeast-ConnectionScheduling)

    在集群系统中的服务器性能差异较大的情况下,调度器采用"加权最少链接"
    调度算法优化负载均衡性能,具有较高权值的服务器将承受较大比例的活动连接负载｡调度器可以自动问询真实服务器的负载情况,并动态地调整其权值

 5.基于局部性的最少链接(Locality-BasedLeastConnectionsScheduling)

    基于局部性的最少链接"调度算法是针对目标IP地址的负载均衡,
    目前主要用于Cache集群系统｡该算法根据请求的目标IP地址找出该目标IP地址最近使用的服务器,
    若该服务器是可用的且没有超载,将请求发送到该服务器;若服务器不存在,或者该服务器超载且有服务器处于一
    半的工作负载,则用"最少链接"的原则选出一个可用的服务器,将请求发送到该服务器｡

 6.带复制的基于局部性最少链接(Locality-BasedLeastConnectionswithReplicationScheduling)

    带复制的基于局部性最少链接"调度算法也是针对目标IP地址的负载均衡,目前主要用于Cache集群系统｡它与LBLC算法的不同之处是它要维护从一个目标IP地址到一组服务器的映射,而LBLC算法维护从一个目标IP地址到一台服务器的映射｡
    该算法根据请求的目标IP地址找出该目标IP地址对应的服务器组,按"最小连接"原则从服务器组中选出一台服务器,若服务器没有超载,将请求发送到该服务器,若服务器超载;则按"最小连接"原则从这个
    集群中选出一台服务器,将该服务器加入到服务器组中,将请求发送到该服务器｡同时,
    当该服务器组有一段时间没有被修改,将最忙的服务器从服务器组中删除,以降低复制的程度

 7.目标地址散列调度(DestinationHashingScheduling)

    目标地址散列"调度算法根据请求的目标IP地址,作为散列键(HashKey)从静态分配的散列表找出对应的服务器,
    若该服务器是可用的且未超载,将请求发送到该服务器,否则返回空

 8.源地址散列调度(SourceHashingScheduling)

    源地址散列"调度算法根据请求的源IP地址,作为散列键(HashKey)从静态分配的
 
  - cafe babe     字节码文件 标识 
  - 0000 0032     版本号 0000次版本号， 0032主版本号 （ 32相当于十进抽的 50）  ---jdk6 编译出来的 


### Nginx  

1.  常用配置参数   
    - include       mime.types;   #文件扩展名与文件类型映射表
    - default_type  application/octet-stream; #默认文件类型，默认为text/plain
    - access_log off; #取消服务日志   
    - log_format myFormat ' $remote_addr–$remote_user [$time_local] $request $status $body_bytes_sent $http_referer $http_user_agent $http_x_forwarded_for'; #自定义格式
    - access_log log/access.log myFormat;  #combined为日志格式的默认值
    - sendfile on;   #允许sendfile方式传输文件，默认为off，可以在http块，server块，location块。
    - sendfile_max_chunk 100k;  #每个进程每次调用传输数量不能大于设定的值，默认为0，即不设上限。
    - keepalive_timeout 65;  #连接超时时间，默认为75s，可以在http，server，location块。
    - proxy_connect_timeout 1;   #nginx服务器与被代理的服务器建立连接的超时时间，默认60秒
    - proxy_read_timeout 1; #nginx服务器想被代理服务器组发出read请求后，等待响应的超时间，默认为60秒。
    - proxy_send_timeout 1; #nginx服务器想被代理服务器组发出write请求后，等待响应的超时间，默认为60秒。
    - proxy_http_version 1.0 ; #Nginx服务器提供代理服务的http协议版本1.0，1.1，默认设置为1.0版本。
      - proxy_method get;    #支持客户端的请求方法。post/get；
    - proxy_ignore_client_abort on;  #客户端断网时，nginx服务器是否终端对被代理服务器的请求。默认为off。
    - proxy_ignore_headers "Expires" "Set-Cookie";  #Nginx服务器不处理设置的http相应投中的头域，这里空格隔开可以设置多个。
    - proxy_intercept_errors on;    #如果被代理服务器返回的状态码为400或者大于400，设置的error_page配置起作用。默认为off。
    - proxy_headers_hash_max_size 1024; #存放http报文头的哈希表容量上限，默认为512个字符。
    - proxy_headers_hash_bucket_size 128; #nginx服务器申请存放http报文头的哈希表容量大小。默认为64个字符。
    - proxy_next_upstream timeout;  #反向代理upstream中设置的服务器组，出现故障时，被代理服务器返回的状态值。error|timeout|invalid_header|http_500|http_502|http_503|http_504|http_404|off
    - proxy_ssl_session_reuse on; 默认为on，如果我们在错误日志中发现“SSL3_GET_FINSHED:digest check failed”的情况时，可以将该指令设置为off。

2. 代理 
    
    ~~~ xml
    upstream mysvr { 
    server 192.168.10.121:3333; 
    server 192.168.10.122:3333; } 
    server { .... location ~*^.+$ { 
    proxy_pass http://mysvr; #请求转向mysvr 定义的服务器列表  } }
    ~~~
    
-  热备：如果你有2台服务器，当一台服务器发生事故时，才启用第二台服务器给提供服务。服务器处理请求的顺序：AAAAAA突然A挂啦，BBBBBBBBBBBBBB.....
    
            ~~~ xml
            upstream mysvr { 
                server 127.0.0.1:7878; 
                server 192.168.10.121:3333 backup; #热备  
            }
            ~~~
    
-  轮询：nginx默认就是轮询其权重都默认为1，服务器处理请求的顺序：ABABABABAB....
  ~~~ xml
            upstream mysvr {
                 server 127.0.0.1:7878; 
                server 192.168.10.121:3333; 
            }
   ~~~
   
- 加权轮询：跟据配置的权重的大小而分发给不同服务器不同数量的请求。如果不设置，则默认为1。下面服务器的请求顺序为：ABBABBABBABBABB....
 ~~~ xml
        upstream mysvr { 
           server 127.0.0.1:7878 weight=1; 
            server 192.168.10.121:3333 weight=2; 
        }
   ~~~
   
- ip_hash:nginx会让相同的客户端ip请求相同的服务器。
 ~~~ xml
        upstream mysvr {
          server 127.0.0.1:7878; 
         server 192.168.10.121:3333; ip_hash; 
        }
   ~~~
        
- 如果你对上面4种均衡算法不是很理解，可以查看Nginx 配置详解，可能会更加容易理解点。

    到这里你是不是感觉nginx的负载均衡配置特别简单与强大，那么还没完，咱们继续哈，这里扯下蛋。

    关于nginx负载均衡配置的几个状态参数讲解。
    down，表示当前的server暂时不参与负载均衡。
    backup，预留的备份机器。当其他所有的非backup机器出现故障或者忙的时候，才会请求backup机器，因此这台机器的压力最轻。
    max_fails，允许请求失败的次数，默认为1。当超过最大次数时，返回proxy_next_upstream 模块定义的错误。
    fail_timeout，在经历了max_fails次失败后，暂停服务的时间。max_fails可以和fail_timeout一起使用。

 ~~~ xml
    upstream mysvr { 
        server 127.0.0.1:7878 weight=2 max_fails=2 fail_timeout=2; 
        server 192.168.10.121:3333 weight=1 max_fails=2 fail_timeout=1; 
    }

   ~~~
 
 
  
  
# License

* [Wiki]()

[]:https://wwww.pigasuo.com





























